import httpx
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import datetime

# ×“×¤×™ ×©×§×™×¤×•×ª ××—×™×¨×™× ×××™×ª×™×™×
CHAIN_PAGES = {
    "good_pharm": "https://goodpharm.binaprojects.com/Main.aspx",
    "zol_vebegadol": "https://zolvebegadol.binaprojects.com/Main.aspx",
    "hazi_hinam": "https://shop.hazi-hinam.co.il/Prices"
}

# ××™×œ×• ×¡×•×’×™ ×§×‘×¦×™× ×× ×—× ×• ××—×¤×©×™×
VALID_KEYWORDS = ["PriceFull", "PriceUpdate", "Promo", "Price", "Full", "Update"]
VALID_EXTENSIONS = [".gz", ".zip", ".xml"]


async def fetch_html(url: str):
    """××•×¨×™×“ HTML ×¢× headers ×›×“×™ ×œ×¢×§×•×£ ×—×¡×™××•×ª."""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
        "Accept": "*/*",
        "Accept-Language": "he-IL,he;q=0.9,en-US;q=0.8"
    }
    async with httpx.AsyncClient(headers=headers, follow_redirects=True, timeout=30.0) as client:
        resp = await client.get(url)
        resp.raise_for_status()
        return resp.text


async def get_latest_file_url(source_id: str):
    """
    ×¡×•×¨×§ ×“×£ ×©×§×™×¤×•×ª ××—×™×¨×™× ×•××—×–×™×¨ ××ª ×”×§×•×‘×¥ ×”×›×™ ×—×“×©.
    ××•×ª×× ×œ-GoodPharm / Zolvebegadol / HaziHinam.
    """
    base_url = CHAIN_PAGES.get(source_id)
    if not base_url:
        print(f"âš ï¸ ××™×Ÿ ×“×£ ××•×’×“×¨ ×¢×‘×•×¨ {source_id}")
        return None

    try:
        html = await fetch_html(base_url)
        soup = BeautifulSoup(html, "html.parser")

        links = []

        # ×—×™×¤×•×© ×‘×›×œ ×”×œ×™× ×§×™× ×‘×¢××•×“
        for a in soup.find_all("a", href=True):
            href = a["href"]

            # ×‘×“×™×§×” ×× ×”×œ×™× ×§ ××›×™×œ ××™×œ×•×ª ××¤×ª×— ×©×œ ×§×‘×¦×™ ××—×™×¨×•×Ÿ
            if any(key in href for key in VALID_KEYWORDS) and any(ext in href for ext in VALID_EXTENSIONS):
                full_url = urljoin(base_url, href)
                links.append(full_url)

        # ×—×¦×™ ×—×™× × â€” ×“×•×¨×© ×ª××¨×™×š
        if source_id == "hazi_hinam" and not links:
            today = datetime.date.today().strftime("%Y-%m-%d")
            dated_url = f"https://shop.hazi-hinam.co.il/Prices?date={today}"
            print(f"ğŸ“… ×—×¦×™ ×—×™× × â€” ×˜×•×¢×Ÿ ×“×£ ×¢× ×ª××¨×™×š: {dated_url}")

            html = await fetch_html(dated_url)
            soup = BeautifulSoup(html, "html.parser")

            for a in soup.find_all("a", href=True):
                href = a["href"]
                if any(key in href for key in VALID_KEYWORDS) and any(ext in href for ext in VALID_EXTENSIONS):
                    full_url = urljoin(dated_url, href)
                    links.append(full_url)

        if not links:
            print(f"âš ï¸ ×œ× × ××¦××• ×§×‘×¦×™ ××—×™×¨×•×Ÿ ×‘-{source_id}")
            return None

        # ××™×•×Ÿ ×œ×¤×™ ×©× ×”×§×•×‘×¥ (×©×‘×“"×› ××›×™×œ ×ª××¨×™×š)
        links.sort(reverse=True)
        latest = links[0]

        print(f"ğŸ¯ Hunter: × ××¦× ×§×•×‘×¥ ×˜×¨×™ ×¢×‘×•×¨ {source_id}: {latest}")
        return latest

    except Exception as e:
        print(f"âŒ ×©×’×™××” ×‘×¡×¨×™×§×ª {source_id}: {e}")
        return None