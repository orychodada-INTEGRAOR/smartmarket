import httpx
from bs4 import BeautifulSoup
from urllib.parse import urljoin

# ×“×¤×™ ×©×§×™×¤×•×ª ××—×™×¨×™× ×××™×ª×™×™×
CHAIN_PAGES = {
    "good_pharm": "https://goodpharm.binaprojects.com/Main.aspx",
    "laib": "https://laibcatalog.co.il/",
    "zol_vebegadol": "https://zolvebegadol.binaprojects.com/Main.aspx",
    "hazi_hinam": "https://shop.hazi-hinam.co.il/Prices"
}

# ××™×œ×• ×¡×•×’×™ ×§×‘×¦×™× ×× ×—× ×• ××—×¤×©×™×
VALID_KEYWORDS = ["PriceFull", "PriceUpdate", "Promo", "Price", "Full", "Update"]
VALID_EXTENSIONS = [".gz", ".zip", ".xml"]


async def get_latest_file_url(source_id: str):
    """
    ×¡×•×¨×§ ×“×£ ×©×§×™×¤×•×ª ××—×™×¨×™× ×•××—×–×™×¨ ××ª ×”×§×•×‘×¥ ×”×›×™ ×—×“×©.
    ××•×ª×× ×œ×“×¤×™× ×›××• GoodPharm / ×–×•×œ ×•×‘×’×“×•×œ / ×—×¦×™ ×—×™× ×.
    """
    base_url = CHAIN_PAGES.get(source_id)
    if not base_url:
        print(f"âš ï¸ ××™×Ÿ ×“×£ ××•×’×“×¨ ×¢×‘×•×¨ {source_id}")
        return None

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
        "Accept": "*/*",
        "Accept-Language": "he-IL,he;q=0.9,en-US;q=0.8"
    }

    async with httpx.AsyncClient(headers=headers, follow_redirects=True, timeout=30.0) as client:
        try:
            response = await client.get(base_url)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, "html.parser")

            links = []

            # ×—×™×¤×•×© ×‘×›×œ ×”×œ×™× ×§×™× ×‘×¢××•×“
            for a in soup.find_all("a", href=True):
                href = a["href"]

                # ×‘×“×™×§×” ×× ×”×œ×™× ×§ ××›×™×œ ××™×œ×•×ª ××¤×ª×— ×©×œ ×§×‘×¦×™ ××—×™×¨×•×Ÿ
                if any(key in href for key in VALID_KEYWORDS) and any(ext in href for ext in VALID_EXTENSIONS):
                    full_url = urljoin(base_url, href)
                    links.append(full_url)

            if not links:
                print(f"âš ï¸ ×œ× × ××¦××• ×§×‘×¦×™ ××—×™×¨×•×Ÿ ×‘-{source_id}")
                return None

            # ××™×•×Ÿ ×œ×¤×™ ×©× ×”×§×•×‘×¥ (×©×‘×“"×› ××›×™×œ ×ª××¨×™×š)
            links.sort(reverse=True)
            latest = links[0]

            print(f"ğŸ¯ Hunter: × ××¦× ×§×•×‘×¥ ×˜×¨×™ ×¢×‘×•×¨ {source_id}: {latest}")
            return latest

        except Exception as e:
            print(f"âŒ ×©×’×™××” ×‘×¡×¨×™×§×ª {source_id}: {e}")
            return None